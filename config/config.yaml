# General
general:
  test_id: "---" # test name
  test_type: TL # type of test to perform (either triplet loss minimization "TL", multi-class classification "MCC", or siamese networks "SN")
  seed: 1 # global seed 

# Model parameters.
model:
  feature_extractor_arch: "resnet18" # backbone model architecture
  num_classes: 23 # number of training classes
  emb_width: 1024 # embedding width
  pretraining: obow # pretraining type (either "obow", "moco", "simclr", "imagenet", "mcc", or None)
  mode: frozen # set "frozen" to freeze the backbone model weights
  cp_path: ./path-to-pretrained-model/pretrained-model.pth # insert the path to the pretrained encoder checkpoints (except for "imagenet" pretraining)
  alpha: False # boolean parameter to add a trainable parameter "alpha" which regulates the embedding magnitude; if "False", the parameter is fixed to "alpha_value"
  alpha_value: 1.0 # initial value of the alpha parameter
  emb_type: "avg" # when adopting the fully convolutional network strategy, set to "avg" to produce a global embedding for the whole page, or "sampling" to sample "samples" embeddings
  samples: 10 # number of samples to sample when "emb_type" is set to "sampling"

# Optimization parameters.
optim:
  optim_type: "sgd" # optimizer (either "sgd" or "adam")
  # optimizer and scheduler parameters
  momentum: 0.9
  weight_decay: 0.0001
  nesterov: False
  num_epochs: 100 # number of epochs
  lr: 0.6
  beta: [0.9, 0.999]
  end_lr: 0.0015
  lr_schedule_type: "cos_warmup" # scheduler type (either "cos_warmup", "step_lr", "exp", or "red_on_plateau")
  step: 10
  gamma: 0.1
  patience: 10
  warmup_epochs: 10
  warmup_start_lr: 0.15
  alpha_min: 1.0 # minimum value of the alpha parameter
  alpha_max: 5.0 # maximum value of the alpha parameter
  loss:
    loss_type: bhtl # set either to "bhtl" or "pml_bhtl" to choose the online-triplet-loss or the PyTorch-Metric-Learning (PML) implementation of the triplet margin loss
    margin: 0.2
    squared: False
    mining: hard
    distance: LpDistance # distance type - valid for PML implementation only ("LpDistance" or "DotProductSimilarity")
    normalize_embedding: True # when "True", the embeddings are L2-normalized
    p: 2
    power: 1
    ia_c_var_min: False # when "True", an intra-class variance minimization term is added to the loss
    iacvm_magnitude: 10 # coefficient of the intra-class variance minimization term
  same_authors: False

# Data parameters:
data:
  batch_size: 256 # batch size
  transforms: # torchvision.transforms parameters ---> https://pytorch.org/vision/stable/transforms.html
    img_crop_size: 380
    rc_p: 1.0 # set either to 1 or 0 to select data augmentation scheme A or B respectively
    cjitter:
      'brightness': [0.4, 1.3]
      'contrast': 0.6 
      'saturation': 0.6
      'hue': 0.4
    cjitter_p: 1
    randaffine:
      'degrees': [-10,10]
      'translate': [0.2, 0.2]
      'scale': [1.3, 1.4]
      'shear': 1
    randpersp:
      'distortion_scale': 0.1
      'p': 0.2
    gray_p: 0.2
    gaussian_blur:
      'kernel_size': 3
      'sigma': [0.1, 0.5]
    rand_eras:
      'p': 0.5
      'scale': [0.02, 0.33]
      'ratio': [0.3, 3.3]
      'value': 0
    invert_p: 0.05
    gaussian_noise:
      'mean': 0.0
      'std': 0.004
    gn_p: 0.0
    n_test_crops: 10
  weighted_sampling: False # when "True", a sampler is added to the dataloader aimed at tackling class imbalance

# Test parameters
test:
  ratio_train: 50 # percentage of the background samples involved in the MAP computation
  ratio_val: 100 # percentage of the evaluation samples involved in the MAP computation